---
name: "Run Unit Tests (Reusable)"

'on':
  workflow_call:
    inputs:
      service:
        description: 'Service to test'
        required: true
        type: string
      environment:
        description: 'Environment context (dev/qa/prod)'
        required: false
        type: string
        default: 'dev'
    outputs:
      tests_passed:
        description: "Whether tests passed"
        value: ${{ jobs.run-tests.outputs.success }}
      test_count:
        description: "Number of tests run"
        value: ${{ jobs.run-tests.outputs.test_count }}

permissions:
  contents: read
  checks: write

jobs:
  run-tests:
    name: Test ${{ inputs.service }}
    runs-on: ubuntu-latest
    outputs:
      success: ${{ steps.test-result.outputs.success }}
      test_count: ${{ steps.test-result.outputs.count }}

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      # =====================================================================
      # Go Services: frontend, checkoutservice, productcatalogservice, shippingservice
      # =====================================================================
      - name: Setup Go
        if: |
          inputs.service == 'frontend' ||
          inputs.service == 'checkoutservice' ||
          inputs.service == 'productcatalogservice' ||
          inputs.service == 'shippingservice'
        uses: actions/setup-go@v5
        with:
          go-version: '1.21'

      - name: Run Go Tests
        id: go-tests
        if: |
          inputs.service == 'frontend' ||
          inputs.service == 'checkoutservice' ||
          inputs.service == 'productcatalogservice' ||
          inputs.service == 'shippingservice'
        working-directory: src/${{ inputs.service }}
        run: |
          # Run tests with JUnit XML output using gotestsum
          go install gotest.tools/gotestsum@latest
          gotestsum --junitfile test-results.xml --format testname -- -v ./...
        continue-on-error: true

      # =====================================================================
      # C# Service: cartservice (xunit)
      # =====================================================================
      - name: Setup .NET
        if: inputs.service == 'cartservice'
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '9.0.x'

      - name: Run C# Tests
        id: csharp-tests
        if: inputs.service == 'cartservice'
        working-directory: src/cartservice
        run: |
          # Run xunit tests with JUnit XML output
          dotnet test tests/cartservice.tests.csproj \
            --logger "junit;LogFilePath=test-results.xml" \
            --verbosity normal
        continue-on-error: true

      # =====================================================================
      # Java Service: adservice (JUnit)
      # =====================================================================
      - name: Setup Java
        if: inputs.service == 'adservice'
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '19'
          cache: 'gradle'
          cache-dependency-path: 'src/${{ inputs.service }}/*.gradle*'

      - name: Run Java Tests
        id: java-tests
        if: inputs.service == 'adservice'
        working-directory: src/adservice
        run: |
          # Run gradle tests (JUnit)
          ./gradlew test --no-daemon
          # Copy test results to expected location
          mkdir -p test-results
          cp build/test-results/test/*.xml test-results/ || true
        continue-on-error: true

      # =====================================================================
      # Python Services: emailservice, recommendationservice, shoppingassistantservice
      # =====================================================================
      - name: Setup Python
        if: |
          inputs.service == 'emailservice' ||
          inputs.service == 'recommendationservice' ||
          inputs.service == 'shoppingassistantservice'
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Run Python Tests
        id: python-tests
        if: |
          inputs.service == 'emailservice' ||
          inputs.service == 'recommendationservice' ||
          inputs.service == 'shoppingassistantservice'
        working-directory: src/${{ inputs.service }}
        run: |
          # Install dependencies
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-xdist

          # Run pytest with JUnit XML output
          pytest --junitxml=test-results.xml \
                 --cov=. \
                 --cov-report=xml \
                 --cov-report=term \
                 -v \
                 || true
        continue-on-error: true

      # =====================================================================
      # Node.js Services: currencyservice, paymentservice
      # Note: These services currently have no tests ("Error: no test specified")
      # =====================================================================
      - name: Setup Node.js
        if: |
          inputs.service == 'currencyservice' ||
          inputs.service == 'paymentservice'
        uses: actions/setup-node@v4
        with:
          node-version: '20'  # Use LTS for better compatibility with native modules
          cache: 'npm'
          cache-dependency-path: 'src/${{ inputs.service }}/package-lock.json'

      - name: Run Node.js Tests (Placeholder)
        id: nodejs-tests
        if: |
          inputs.service == 'currencyservice' ||
          inputs.service == 'paymentservice'
        working-directory: src/${{ inputs.service }}
        run: |
          npm install
          # Create empty test result for services without tests
          mkdir -p test-results
          cat > test-results.xml <<'EOF'
          <?xml version="1.0" encoding="UTF-8"?>
          <testsuites>
            <testsuite name="${{ inputs.service }}" tests="0" failures="0" errors="0" skipped="0">
              <properties>
                <property name="status" value="no tests configured"/>
              </properties>
            </testsuite>
          </testsuites>
          EOF
          echo "âš ï¸ No tests configured for ${{ inputs.service }}"
        continue-on-error: true

      # =====================================================================
      # Services with no tests: loadgenerator (load testing tool, not unit tests)
      # =====================================================================
      - name: Create Placeholder Test Results
        if: inputs.service == 'loadgenerator'
        run: |
          mkdir -p src/${{ inputs.service }}/test-results
          cat > src/${{ inputs.service }}/test-results.xml <<'EOF'
          <?xml version="1.0" encoding="UTF-8"?>
          <testsuites>
            <testsuite name="${{ inputs.service }}" tests="0" failures="0" errors="0" skipped="0">
              <properties>
                <property name="status" value="load testing tool - no unit tests"/>
              </properties>
            </testsuite>
          </testsuites>
          EOF

      # =====================================================================
      # Find and verify test results
      # =====================================================================
      - name: Locate Test Results
        id: find-results
        run: |
          SERVICE_DIR="src/${{ inputs.service }}"

          # Search for test result XML files
          TEST_FILE=""
          if [ -f "$SERVICE_DIR/test-results.xml" ]; then
            TEST_FILE="$SERVICE_DIR/test-results.xml"
          elif [ -d "$SERVICE_DIR/test-results" ]; then
            TEST_FILE="$SERVICE_DIR/test-results"
          elif [ -d "$SERVICE_DIR/build/test-results/test" ]; then
            TEST_FILE="$SERVICE_DIR/build/test-results/test"
          elif [ -d "$SERVICE_DIR/tests/TestResults" ]; then
            TEST_FILE="$SERVICE_DIR/tests/TestResults"
          fi

          if [ -z "$TEST_FILE" ]; then
            echo "âš ï¸ No test results found for ${{ inputs.service }}"
            echo "found=false" >> $GITHUB_OUTPUT
          else
            echo "âœ… Test results found: $TEST_FILE"
            echo "found=true" >> $GITHUB_OUTPUT
            echo "path=$TEST_FILE" >> $GITHUB_OUTPUT
          fi

      # =====================================================================
      # Upload Test Results to ServiceNow
      # =====================================================================
      - name: Upload Test Results to ServiceNow
        if: steps.find-results.outputs.found == 'true'
        uses: ServiceNow/servicenow-devops-test-report@v6.0.0
        with:
          devops-integration-user-name: ${{ secrets.SERVICENOW_USERNAME }}
          devops-integration-user-password: ${{ secrets.SERVICENOW_PASSWORD }}
          instance-url: ${{ secrets.SERVICENOW_INSTANCE_URL }}
          tool-id: ${{ secrets.SN_ORCHESTRATION_TOOL_ID }}
          context-github: ${{ toJSON(github) }}
          job-name: 'Test ${{ inputs.service }}'
          xml-report-filename: ${{ steps.find-results.outputs.path }}
        continue-on-error: true

      # =====================================================================
      # Publish Test Results to GitHub
      # =====================================================================
      - name: Publish Test Results
        if: always() && steps.find-results.outputs.found == 'true'
        uses: EnricoMi/publish-unit-test-result-action@v2
        with:
          files: ${{ steps.find-results.outputs.path }}/**/*.xml
          check_name: "${{ inputs.service }} Test Results"
        continue-on-error: true

      # =====================================================================
      # Summary and Output
      # =====================================================================
      - name: Test Result Summary
        id: test-result
        if: always()
        run: |
          echo "### ðŸ§ª ${{ inputs.service }} Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Determine test outcome
          if [ "${{ steps.go-tests.outcome }}" == "success" ] || \
             [ "${{ steps.csharp-tests.outcome }}" == "success" ] || \
             [ "${{ steps.java-tests.outcome }}" == "success" ] || \
             [ "${{ steps.python-tests.outcome }}" == "success" ] || \
             [ "${{ steps.nodejs-tests.outcome }}" == "success" ]; then
            echo "success=true" >> $GITHUB_OUTPUT
            echo "âœ… **Status**: Tests passed" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ steps.find-results.outputs.found }}" == "false" ]; then
            echo "success=true" >> $GITHUB_OUTPUT
            echo "âš ï¸ **Status**: No tests found (skipped)" >> $GITHUB_STEP_SUMMARY
          else
            echo "success=false" >> $GITHUB_OUTPUT
            echo "âŒ **Status**: Tests failed" >> $GITHUB_STEP_SUMMARY
          fi

          # Count tests (approximate from XML if available)
          TEST_COUNT=0
          if [ -f "${{ steps.find-results.outputs.path }}" ]; then
            TEST_COUNT=$(grep -o 'tests="[0-9]*"' "${{ steps.find-results.outputs.path }}" | head -1 | grep -o '[0-9]*' || echo "0")
          fi
          echo "count=$TEST_COUNT" >> $GITHUB_OUTPUT

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Tests Run**: $TEST_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "**Test Framework**: Go Test / xUnit / JUnit / pytest / Jest" >> $GITHUB_STEP_SUMMARY
          echo "**Environment**: \`${{ inputs.environment }}\`" >> $GITHUB_STEP_SUMMARY

          if [ "${{ steps.find-results.outputs.found }}" == "true" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "âœ… Test results uploaded to ServiceNow" >> $GITHUB_STEP_SUMMARY
            echo "âœ… Test results published to GitHub Checks" >> $GITHUB_STEP_SUMMARY
          fi
